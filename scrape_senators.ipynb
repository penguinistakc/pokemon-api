{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Scraping U.S. Senator Data from Wikipedia\n",
    "\n",
    "This notebook demonstrates using `requests` and `BeautifulSoup` to scrape structured data\n",
    "from a Wikipedia page, and using `re` to clean up the extracted text.\n",
    "\n",
    "We will:\n",
    "1. Fetch the HTML of the [List of current United States senators](https://en.wikipedia.org/wiki/List_of_current_United_States_senators)\n",
    "2. Parse the senators table to extract names, states, and party affiliations\n",
    "3. Clean up Wikipedia footnotes using regular expressions\n",
    "4. Visit each senator's individual Wikipedia page to find their official Senate website URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b2c3d4",
   "metadata": {},
   "source": [
    "## Step 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "WIKI_BASE = \"https://en.wikipedia.org\"\n",
    "HEADERS = {\"User-Agent\": \"PYT200-064 Class Demo (educational project)\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2c3d4",
   "metadata": {},
   "source": [
    "## Step 2: Fetch the Senators List Page\n",
    "\n",
    "Wikipedia requires a `User-Agent` header — requests without one get a **403 Forbidden** error.\n",
    "This is common with many websites and is one of the first real-world obstacles you'll encounter when scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"{}/wiki/List_of_current_United_States_senators\".format(WIKI_BASE)\n",
    "response = requests.get(url, headers=HEADERS)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "print(\"Page fetched: {} characters\".format(len(response.text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b2c3d4",
   "metadata": {},
   "source": [
    "## Step 3: Find the Senators Table\n",
    "\n",
    "The page has **four** sortable tables — the first three are leadership summaries.\n",
    "The senators table is the fourth one. Always inspect what `find_all` returns\n",
    "rather than assuming there's only one match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = soup.find_all(\"table\", class_=\"sortable\")\n",
    "print(\"Found {} sortable tables\".format(len(tables)))\n",
    "\n",
    "# Show what each table looks like by its header row\n",
    "for i, t in enumerate(tables):\n",
    "    header_row = t.find(\"tr\")\n",
    "    headers = [th.get_text(strip=True)[:20] for th in header_row.find_all(\"th\")[:5]]\n",
    "    print(\"  Table {}: {}\".format(i, headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = tables[3]\n",
    "tbody = table.find(\"tbody\")\n",
    "rows = tbody.find_all(\"tr\")\n",
    "print(\"Senator table has {} rows (1 header + {} data rows)\".format(len(rows), len(rows) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1b2c3d4",
   "metadata": {},
   "source": [
    "## Step 4: Understand the Row Structure\n",
    "\n",
    "The table uses `rowspan=\"2\"` on state cells, meaning each state name spans two rows\n",
    "(one per senator). The second senator's row has **no state cell** — it's covered by the rowspan.\n",
    "\n",
    "Senator names are in `<th>` elements (row headers), not `<td>`. This is semantically\n",
    "correct HTML but surprises students who expect all data in `<td>` elements.\n",
    "\n",
    "Let's inspect the first few rows to see this structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first 4 data rows to see the rowspan pattern\n",
    "for row in rows[1:5]:\n",
    "    cells = row.find_all([\"td\", \"th\"])\n",
    "    for cell in cells[:5]:\n",
    "        tag = cell.name\n",
    "        rowspan = cell.get(\"rowspan\", \"-\")\n",
    "        text = cell.get_text(strip=True)[:25]\n",
    "        print(\"  <{}> rowspan={}: \\\"{}\\\"\".format(tag, rowspan, text))\n",
    "    print(\"  ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1b2c3d4",
   "metadata": {},
   "source": [
    "## Step 5: Parse All Senators\n",
    "\n",
    "We loop through every row, tracking the current state. When we see a `<td>` with\n",
    "`rowspan=\"2\"`, we know a new state has started. Senator names come from `<th>` elements.\n",
    "\n",
    "For the party column, we navigate relative to the `<th>` using `find_next_sibling`\n",
    "rather than counting column positions (which shift due to the rowspan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": "senators = []\ncurrent_state = None\n\nfor row in rows:\n    # State cells have rowspan=\"2\", spanning both senator rows\n    state_cell = row.find(\"td\", rowspan=\"2\")\n    if state_cell:\n        current_state = state_cell.get_text(strip=True)\n\n    # Senator names are in <th> row header elements\n    name_cell = row.find(\"th\")\n    if name_cell and current_state:\n        name = name_cell.get_text(strip=True)\n        wiki_link = name_cell.find(\"a\")\n        wiki_path = wiki_link[\"href\"] if wiki_link else \"\"\n\n        # Party is two <td> siblings after the <th>: an empty color cell, then the party name\n        party_cell = name_cell.find_next_sibling(\"td\").find_next_sibling(\"td\")\n\n        # Extract footnote references before getting the clean party text\n        notes = \"\"\n        footnotes = party_cell.find_all(\"sup\")\n        if footnotes:\n            for sup in footnotes:\n                link = sup.find(\"a\")\n                if link and link.get(\"href\", \"\").startswith(\"#cite_note\"):\n                    ref_id = link[\"href\"].lstrip(\"#\")\n                    ref_li = soup.find(\"li\", id=ref_id)\n                    if ref_li:\n                        ref_text = ref_li.find(\"span\", class_=\"reference-text\")\n                        if ref_text:\n                            notes = ref_text.get_text(\" \", strip=True)\n                            # Use re.sub to remove bracketed citation numbers like [15]\n                            notes = re.sub(r\"\\s*\\[\\s*\\d+\\s*\\]\", \"\", notes).strip()\n                sup.decompose()  # Remove <sup> so it doesn't appear in party text\n\n        party = party_cell.get_text(strip=True)\n        senators.append((name, current_state, party, wiki_path, notes))\n\nprint(\"Parsed {} senators\".format(len(senators)))"
  },
  {
   "cell_type": "markdown",
   "id": "m1b2c3d4",
   "metadata": {},
   "source": [
    "## Step 6: Display the Results\n",
    "\n",
    "Let's see what we've collected so far — senator names, states, and parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:<30} {:<20} {}\".format(\"Senator\", \"State\", \"Party\"))\n",
    "print(\"-\" * 65)\n",
    "for name, state, party, wiki_path, notes in senators:\n",
    "    print(\"{:<30} {:<20} {}\".format(name, state, party))\n",
    "print(\"\\nTotal: {} senators\".format(len(senators)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o1b2c3d4",
   "metadata": {},
   "source": [
    "## Step 7: A Closer Look at `re.sub` for Cleaning Footnotes\n",
    "\n",
    "Four senators have Wikipedia footnotes in their party cells. The footnote text\n",
    "contains bracketed citation numbers like `[15]` that we need to strip.\n",
    "\n",
    "The regex pattern `\\s*\\[\\s*\\d+\\s*\\]` matches:\n",
    "\n",
    "| Component | Matches |\n",
    "|---|---|\n",
    "| `\\s*` | Optional whitespace before the bracket |\n",
    "| `\\[` | Literal `[` (escaped because `[` is special in regex) |\n",
    "| `\\s*` | Optional whitespace inside the bracket |\n",
    "| `\\d+` | One or more digits |\n",
    "| `\\s*` | Optional whitespace before closing bracket |\n",
    "| `\\]` | Literal `]` |\n",
    "\n",
    "Let's see which senators have notes and what the cleanup looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, state, party, wiki_path, notes in senators:\n",
    "    if notes:\n",
    "        print(\"{} ({})\".format(name, state))\n",
    "        print(\"  Party: {}\".format(party))\n",
    "        print(\"  Note:  {}\".format(notes))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q1b2c3d4",
   "metadata": {},
   "source": [
    "## Step 8: Fetch Senate Website URLs\n",
    "\n",
    "Each senator's name links to their individual Wikipedia page (e.g., `/wiki/Katie_Britt`).\n",
    "On that page, an **infobox** sidebar contains a \"Website\" row with their official Senate URL.\n",
    "\n",
    "We define a function to fetch one senator's website, then use `ThreadPoolExecutor`\n",
    "to fetch all 100 pages in parallel — roughly 10x faster than doing them one at a time.\n",
    "\n",
    "Threads work well here because the task is **I/O-bound** (waiting for Wikipedia to respond).\n",
    "For **CPU-bound** work, you would use `ProcessPoolExecutor` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senate_website(wiki_path):\n",
    "    \"\"\"Fetch a senator's Wikipedia page and extract the Senate website URL from the infobox.\"\"\"\n",
    "    url = \"{}{}\".format(WIKI_BASE, wiki_path)\n",
    "    resp = requests.get(url, headers=HEADERS)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    page_soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    infobox = page_soup.find(\"table\", class_=\"infobox\")\n",
    "    if infobox:\n",
    "        for th in infobox.find_all(\"th\"):\n",
    "            if \"Website\" in th.get_text():\n",
    "                td = th.find_next_sibling(\"td\")\n",
    "                if td:\n",
    "                    link = td.find(\"a\")\n",
    "                    if link:\n",
    "                        return link.get(\"href\", \"\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fetching website URLs for {} senators...\".format(len(senators)))\n",
    "websites = [\"\"] * len(senators)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    future_to_index = {}\n",
    "    for i, senator in enumerate(senators):\n",
    "        if senator[3]:  # only if we have a wiki path\n",
    "            future = executor.submit(get_senate_website, senator[3])\n",
    "            future_to_index[future] = i\n",
    "\n",
    "    for future in as_completed(future_to_index):\n",
    "        idx = future_to_index[future]\n",
    "        try:\n",
    "            websites[idx] = future.result()\n",
    "        except Exception as e:\n",
    "            print(\"  Warning: could not fetch website for {}: {}\".format(senators[idx][0], e))\n",
    "\n",
    "# Combine the website URLs with the senator data\n",
    "senators = [\n",
    "    (name, state, party, websites[i], notes)\n",
    "    for i, (name, state, party, _, notes) in enumerate(senators)\n",
    "]\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t1b2c3d4",
   "metadata": {},
   "source": [
    "## Step 9: Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:<30} {:<20} {:<15} {}\".format(\"Senator\", \"State\", \"Party\", \"Website\"))\n",
    "print(\"-\" * 110)\n",
    "for name, state, party, website, notes in senators:\n",
    "    print(\"{:<30} {:<20} {:<15} {}\".format(name, state, party, website))\n",
    "print(\"\\nTotal: {} senators\".format(len(senators)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}